{"cells":[{"metadata":{},"cell_type":"markdown","source":"https://wikidocs.net/21698"},{"metadata":{},"cell_type":"markdown","source":"# 한국어 전처리 패키지\nhttps://wikidocs.net/92961\n\n- PyKoSpacing\n    - !pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n    - 띄워쓰기 안한 문장을 띄워쓰기를 한 문장으로 변환시켜주는 패키지\n- Py-Hanspell\n    - !pip install git+https://github.com/ssut/py-hanspell.git\n    - 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 맞춤법 수정 패키지 + 띄워쓰기 수정지원\n- SOYNLP를 이용한 단어 토큰화\n    - !pip install soynlp\n    - 학습을 거쳐서 그것을 기반으로한 형태소 분석을 진행\n    - 반복되는 문자 정제 지원 (ㅋㅋㅋㅋ, ㅠㅠㅠㅠ, 하하하하하)\n    - https://github.com/lovit/soynlp\n    - 응집확률을 계산\n    - 브랜칭 엔트로피값 사용\n- Customized KoNLPy\n    - !pip install customized_konlpy\n    - add_dictionary 를 통해서 형태소 조정 가능\n    - ex) '은경이' 를 '은' + '경이' 로 인식할때 '은경이'를 추가해줌"},{"metadata":{},"cell_type":"markdown","source":"# 텍스트 전처리"},{"metadata":{},"cell_type":"markdown","source":"## 토큰화"},{"metadata":{},"cell_type":"markdown","source":"- 토큰의 단위는 상황에 따라 다르지만 보통 의미있는 단위로 토큰을 정의합니다.\n    - 단어 토큰화\n    - 문장 토큰화"},{"metadata":{},"cell_type":"markdown","source":"### 토큰화에서 주의점\n\n- 구두점이나 특수문자를 단순제외해서는 안됩니다.\n    - 달러($), 혹은 줄임말 (L.O.L) 의 경우\n    - 45.5cm 의 경우\n    - 1,000 원의 경우처럼 수치 표현\n\n- 줄임말과 단어내 띄워쓰기의 경우\n    - what's = what is, we're = we are\n    - rock n roll = 락앤롤 이라는 하나의 단어지만 띄워쓰기 됨"},{"metadata":{},"cell_type":"markdown","source":"### 표준토큰화\n\n- Penn Treebank Tokenization\n    - 하이픈으로 구성된 단어는 하나로 유지\n    - doesn't 와 같은 단어는 분리 => does not"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 영어 sentence_tokenize\n# from nltk.tokenize import word_tokenize  #단어 토큰화 라이브러리\nfrom nltk.tokenize import sent_tokenize\n\ntext=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\ntext2=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n\nprint(sent_tokenize(text))\nprint(sent_tokenize(text2))\n# 문장 토큰화에서 단순히 온점으로만 구분하지 않기때문에 \"I am actively looking for Ph.D. students.\"\n# Ph.D. 에서 문장이 구분되지 않았다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kss # 한글 sentence_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import kss\n\ntext_ko='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\nprint(kss.split_sentences(text_ko))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 한국어 토큰화"},{"metadata":{},"cell_type":"markdown","source":"### 조사"},{"metadata":{},"cell_type":"markdown","source":"- 한국어에는 주어나 목적어 뒤에 조사가 붙습니다.\n    - ex) 그 라는 단어 뒤에 많은 조사가 존재합니다.\n        - 그가, 그에게, 그를, 그와, 그는 등의 다양한 조사가 존재합니다.\n- 대부분의 한국어 NLP에서는 조사를 분리해줄 필요가 있습니다.        "},{"metadata":{},"cell_type":"markdown","source":"### 형태소"},{"metadata":{},"cell_type":"markdown","source":"- 가장 작은 말의 단위\n- 자립형태소, 의존형태소 존재\n- 자립형태소 : 접사, 어미, 조사와 상관없이 사용가능한 형태소 그 자체로 단어가 된다.\n    - 체언(명사, 대명사, 수사), 감탄사 , 수식언 등\n- 의존형태소 : 다른 형태소와 결합하여 사용되는 형태소\n    - 접사, 어미, 조사, 어간\n- 에디가 딥러닝책을 읽었다.\n    - 자립형태소 : 에디, 딥러닝책\n    - 의존형태소 : -가, -을, 읽-, -었, -다\n    \n\n**형태소 토큰화를 수행해야 합니다.**"},{"metadata":{},"cell_type":"markdown","source":"### 띄워쓰기"},{"metadata":{},"cell_type":"markdown","source":"- 띄워쓰기를하지않아도글을이해할수있다.\n- canunderstandwithoutspace.\n- 위 처럼 띄워쓰기 없이도 한글은 이해하기 쉽다."},{"metadata":{},"cell_type":"markdown","source":"## 품사태깅"},{"metadata":{},"cell_type":"markdown","source":"- 단어는 표기는 같지만 품사에 따라 단어의 의미가 달라지기도 합니다.\n    - fly: 날다(동사) | 파리(명사)\n    - 못: 할수없다(부사) | 고정하는 물건(명사)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\n\ntext=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\nprint(word_tokenize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=word_tokenize(text)\npos_tag(x)\n# PRP는 인칭 대명사\n# VBP는 동사\n# RB는 부사\n# VBG는 현재부사\n# IN은 전치사\n# NNP는 고유 명사\n# NNS는 복수형 명사\n# CC는 접속사\n# DT는 관사","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install konlpy # 한국어 자연어 처리 라이브러리\n# 형태소분석기\n# Okt(Open Korea Text)\n# 메캅(Mecab)\n# 코모란(Komoran)\n# 한나눔(Hannanum)\n# 꼬꼬마(Kkma)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from konlpy.tag import Okt\n\nokt=Okt()  \nprint(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) # 형태소 추출","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  # 품사 태깅","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  # 명사 추출","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- okt.morphs() 텍스트를 형태소 단위로 나눈다. 옵션으로 norm과 stem이 있다. norm은 문장을 정규화. stem은 각 단어에서 어간을 추출.(기본값은 둘다 False)\n- okt.nouns() 텍스트에서 명사만 뽑아낸다.\n- okt.phrases() 텍스트에서 어절을 뽑아낸다.\n- okt.pos() 각 품사를 태깅하는 역할을 한다. 품사를 태깅한다는 것은 주어진 텍스트를 형태소 단위로 나누고, 나눠진 각 형태소를 그에 해당하는 품사와 함께 리스트화하는 것을 의미한다. 옵션으로 norm, stem, join이 있는데 join은 나눠진 형태소와 품사를 ‘형태소/품사’ 형태로 같이 붙여서 리스트화한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 정제 & 정규화"},{"metadata":{},"cell_type":"markdown","source":"- 정제 : 갖고있는 코퍼스로부터 노이즈 데이터를 제거한다.\n- 정규화 : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다."},{"metadata":{},"cell_type":"markdown","source":"- 정제(cleaning)\n    - 토큰화 이전에 원활한 토큰화를 위해 수행\n    - 토큰화 이후에 남아있는 노이즈 제거를 위해 수행\n    - 완벽한 정제는 어려운편"},{"metadata":{},"cell_type":"markdown","source":"## 규칙에 기반한 통합\n\n\n- USA 와 US 는 같은 의미\n- uh-huh 와 uhhuh 는 같은 의미"},{"metadata":{},"cell_type":"markdown","source":"### 통합하는 방법\n\n- 어간 추출(stemming)\n- 표제어 추출(lemmatizaiton)"},{"metadata":{},"cell_type":"markdown","source":"## 대소문자 통합 (영어)"},{"metadata":{},"cell_type":"markdown","source":"- 무작정 대문자를 소문자로 변환해서는 안됨.\n- 미국이라는 의미의 US와 우리라는 의미의 us는 구별되어야함.\n- 문장 맨앞의 단어만 소문자로 바꾸는 방법도있음.\n- 하지만 인터넷에서 수집했을경우 그냥 모두 소문자로 바꾸는 것도 not bad??"},{"metadata":{},"cell_type":"markdown","source":"## 불필요한 단어의 제거(Removing Unnecessary Words)"},{"metadata":{},"cell_type":"markdown","source":"### 등장 빈도가 적은 단어\n\n- 너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어 존재.\n\n### 길이가 짧은 단어\n\n- 영어권에서는 짧은 단어는 대부분 불용어에 해당해서 제거하면 도움이 된다고함.\n    - a, it, at, to, on, in, by 등과 같은 불용어가 제거된다고 함."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ntext = \"I was wondering if anyone out there could enlighten me on this car.\"\nshortword = re.compile(r'\\W*\\b\\w{1,2}\\b') # 길이가 1,2 제거\n\nprint(shortword.sub('', text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 크롤링을 활용한 데이터의 경우 정규표현식을 통해 html tag등을 제거해주어야합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 어간 추출(Stemming) & 표제어 추출(Lemmatization)"},{"metadata":{},"cell_type":"markdown","source":"- 정규화 기법중 단어의 개수를 줄일 수 있는 기법인 어간추출과 표제어 추출\n- 단어의 모양은 달라도 같은 단어들이라면 하나의 단어로 일반화 시켜서 문서내의 단어수를 줄이겠다는 의미\n- BoW(Bag of Words) 표현을 사용하는 자연어 처리 문제에서 주로 사용됨."},{"metadata":{},"cell_type":"markdown","source":"## 표제어 추출\n\n- am, are, is는 서로 다른 스펠링이지만 이 단어들의 표제어는 be라고 합니다.\n- 형태소는 두 종류가 있습니다.\n    - 어간 : 단어의 의미를 담고 있는 단어의 핵심 부분\n    - 접사 : 단어에 추가적인 의미를 주는 부분"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer # 표제어 추출\n\nn=WordNetLemmatizer()\nwords=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n\nprint([n.lemmatize(w) for w in words])\n# 출력에서는 dy나 ha와 같은 단어들이 보입니다.\n# 이는 정확한 표제어 변환이 아닙니다.\n# 이유는 본래 표제어 추출기가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문입니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(n.lemmatize('dies', 'v'))\nprint(n.lemmatize('watched', 'v'))\nprint(n.lemmatize('has', 'v'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer # 어간 추출\n# PorterStemmer 가 정밀하게 설계되어 정확도가 높다.\n# 영어에서 어간 추출을 하고자 한다면 가장 준수한 선택이다.\n\ns = PorterStemmer()\n\nwords=['formalize', 'allowance', 'electricical']\nprint([s.stem(w) for w in words])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 한국어의 경우엔 예시를 보았지만 조금 복잡해 보여 기록하지않았습니다."},{"metadata":{},"cell_type":"markdown","source":"# 불용어(Stopword)"},{"metadata":{},"cell_type":"markdown","source":"- 불용어는 문장에서 자주 등장하지만 문장의 의미를 분석하는 일에 있어서는 크게 도움되지 않는 단어를 말합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords  \n\nstopwords.words('english')[:10] # 영어 불용어 10개만 보기","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize \n\nexample = \"Family is not an important thing. It's everything.\"\nstop_words = set(stopwords.words('english')) \n\nword_tokens = word_tokenize(example)\n\nresult = []\nfor w in word_tokens: \n    if w not in stop_words: \n        result.append(w) \n\nprint(word_tokens) \nprint(result)\n# is not an 등이 지워짐\n# not은 의미분석하는데 꽤 중요하다고 생각하는데 왜 지워진지 잘 모르겠음","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 한국어 불용어"},{"metadata":{},"cell_type":"markdown","source":"- 한국어 불용어 같은경우 불용어를 직접 작성해서 사용하게 되는경우가 많음"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\nexample = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\nstop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\nstop_words=stop_words.split(' ')\nword_tokens = word_tokenize(example)\n\nresult = [] \nfor w in word_tokens: \n    if w not in stop_words: \n        result.append(w) \n# 위의 4줄은 아래의 한 줄로 대체 가능\n# result=[word for word in word_tokens if not word in stop_words]\n\nprint(word_tokens) \nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 정규표현식\n\n- https://wikidocs.net/21703 참고하여 상황에 맞게 사용할것"},{"metadata":{},"cell_type":"markdown","source":"# 정수 인코딩\n\n- 컴퓨터는 텍스트 보다는 숫자를 더 잘 처리합니다.\n- 텍스트를 숫자로 본격적으로 바꾸기 전에 각 단어를 고유한 정수에 맵핑 시키는 전처리 작업이 필요합니다.\n- 보통은 단어에 대한 빈도수를 기준으로 정렬한 뒤에 부여합니다."},{"metadata":{},"cell_type":"markdown","source":"- dictionary로 하는 방법도 있으나\n- 저자는 Counter, FreqDist, enumerate 또는 케라스 토크나이저 를 사용하는것을 권장하였습니다."},{"metadata":{},"cell_type":"markdown","source":"### Counter"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. 문장 토큰화\ntext = sent_tokenize(text)\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. 정제와 단어 토큰화\nvocab = {} # 파이썬의 dictionary 자료형\nsentences = []\nstop_words = set(stopwords.words('english'))\n\nfor i in text:\n    sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n    result = []\n\n    for word in sentence: \n        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n                result.append(word)\n                if word not in vocab:\n                    vocab[word] = 0 \n                vocab[word] += 1\n    sentences.append(result) \nprint(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 문장토큰화 -> 정제 -> 단어토큰화를 거쳐서\n# 하나의 리스트로 만들어줍니다.\nwords = sum(sentences, [])\n# 위 작업은 words = np.hstack(sentences)로도 수행 가능.\nprint(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = Counter(words) # 파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있습니다.\nprint(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = vocab.most_common(5) # 등장 빈도수가 높은 상위 5개의 단어만 표시\nvocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_to_index = {}\ni = 0\nfor (word, frequency) in vocab :\n    i = i+1\n    word_to_index[word] = i\nprint(word_to_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 텐서플로우 Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\nsentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.word_index)\n# 빈도수가 높은순으로 인덱스가 부여됨","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.word_counts)\n# 각 단어가 몇번 등장했는지 보여줌","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.texts_to_sequences(sentences))\n# 각 단어를 정해진 인덱스로 변환","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 빈도수가 높은 상위 몇개의 단어만 사용하겠다고 지정\nvocab_size = 5\ntokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\ntokenizer.fit_on_texts(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.word_index) # 출력해보면 5개가아니라 전부적용되는데 ???","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.texts_to_sequences(sentences)) # 실제 적용은 texts_to_sequences 때 적용\n# 빈도수 상위 5개가 아니어서 탈락된 OOV 들은 1로 표시됨","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 패딩(Padding)"},{"metadata":{},"cell_type":"markdown","source":"- 자연어 처리에서 문장또는 문서의 길이가 다를 수 있습니다.\n- 이때 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업입니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nsentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = tokenizer.texts_to_sequences(sentences)\nprint(encoded) # 모든 단어가 고유한 정수로 변환되었음","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = max(len(item) for item in encoded)\nmin_len = min(len(item) for item in encoded)\nprint(max_len, min_len)\n# 가장 긴 문장은 7\n# 가장 짧은 문장은 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n# 텐서플로우 에서 지원되는 패딩 도구","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded = pad_sequences(encoded) # default 로 앞에서부터 0을 채움\npadded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded = pad_sequences(encoded, padding = 'post')\npadded # padding에 post 를 주면 뒤에서 부터 0을 채움","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_value = len(tokenizer.word_index) + 1 # 단어 집합의 크기보다 1 큰 숫자를 사용\nprint(last_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded = pad_sequences(encoded, padding = 'post', value = last_value)\npadded # value에 어떤 값을 주면 0이 아닌 다른 숫자로 패딩이 가능","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}